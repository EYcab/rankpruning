{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import errno\n",
    "\n",
    "from rankpruning import RankPruning, other_pnlearning_methods  \n",
    "from util import get_dataset, downsample, get_metrics, make_sure_path_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(key = None, rh1 = None, rh0 = None, clf = None):\n",
    "  models = {\n",
    "    \"Rank Pruning\" : RankPruning(clf = clf),\n",
    "    \"Baseline\" : other_pnlearning_methods.BaselineNoisyPN(clf = clf),\n",
    "    \"True Classifier\": clf,\n",
    "    \"Rank Pruning (noise rates given)\": RankPruning(rh1, rh0, clf = clf),\n",
    "    \"Elk08 (noise rates given)\": other_pnlearning_methods.Elk08(e1 = 1 - rh1, clf = clf),\n",
    "    \"Liu16 (noise rates given)\": other_pnlearning_methods.Liu16(rh1, rh0, clf = clf),\n",
    "    \"Nat13 (noise rates given)\": other_pnlearning_methods.Nat13(rh1, rh0, clf = clf),\n",
    "  } \n",
    "  try:\n",
    "    model = models[key]\n",
    "  except:\n",
    "    model = None\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_test(\n",
    "  dataset,\n",
    "  clf_type, \n",
    "  epochs, \n",
    "  true_rh1,\n",
    "  downsample_ratio, \n",
    "  ordered_models_keys, \n",
    "  list_of_images = range(10), \n",
    "  suppress_error = False,\n",
    "  verbose = False,\n",
    "  pi1 = 0.0,\n",
    "  one_vs_rest = True,\n",
    "  cv_n_folds = 3,\n",
    "  early_stopping = True,\n",
    "  pulearning = None,\n",
    "):\n",
    "\n",
    "  # Cast types to ensure consistency for 1 and 1.0, 0 and 0.0\n",
    "  true_rh1 = float(true_rh1)\n",
    "  downsample_ratio = float(downsample_ratio)\n",
    "  pi1 = float(pi1)\n",
    "\n",
    "  # Load MNIST or CIFAR data\n",
    "  (X_train_original, y_train_original), (X_test_original, y_test_original) = get_dataset(dataset = dataset)\n",
    "  X_train_original, y_train_original = downsample(X_train_original, y_train_original, downsample_ratio)\n",
    "\n",
    "  # Initialize models and result storage\n",
    "  metrics = {key:[] for key in ordered_models_keys}\n",
    "  data_all = {\"metrics\": metrics, \"calculated\": {}, \"errors\": {}}\n",
    "  start_time = dt.now()\n",
    "\n",
    "  # Run through the ten images class of 0, 1, ..., 9\n",
    "  for image in list_of_images:\n",
    "    if one_vs_rest:\n",
    "      # X_train and X_test will not be modified. All data will be used. Adjust pointers.\n",
    "      X_train = X_train_original\n",
    "      X_test = X_test_original\n",
    "\n",
    "      # Relabel the image data. Make label 1 only for given image.\n",
    "      y_train = np.array(y_train_original == image, dtype=int)\n",
    "      y_test = np.array(y_test_original == image, dtype=int)\n",
    "    else: # one_vs_other\n",
    "      # Reducing the dataset to just contain our image and image = 4\n",
    "      other_image = 4 if image != 4 else 7\n",
    "      X_train = X_train_original[(y_train_original == image) | (y_train_original == other_image)]\n",
    "      y_train = y_train_original[(y_train_original == image) | (y_train_original == other_image)]\n",
    "      X_test = X_test_original[(y_test_original == image) | (y_test_original == other_image)]\n",
    "      y_test = y_test_original[(y_test_original == image) | (y_test_original == other_image)]\n",
    "\n",
    "      # Relabel the data. Make label 1 only for given image.\n",
    "      y_train = np.array(y_train == image, dtype=int)\n",
    "      y_test = np.array(y_test == image, dtype=int)\n",
    "\n",
    "    print()\n",
    "    print(\"Evaluating image:\", image)\n",
    "    print(\"Number of positives in y:\", sum(y_train))\n",
    "    print()\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    s = y_train * (np.cumsum(y_train) < (1 - true_rh1) * sum(y_train))\n",
    "    # In the presence of mislabeled negative (negative incorrectly labeled positive):\n",
    "    # pi1 is the fraction of mislabeled negative in the labeled set:\n",
    "    num_mislabeled = int(sum(y_train) * (1 - true_rh1) * pi1 / (1 - pi1))\n",
    "    if num_mislabeled > 0:\n",
    "      negative_set = s[y_train==0]\n",
    "      mislabeled = np.random.choice(len(negative_set), num_mislabeled, replace = False)\n",
    "      negative_set[mislabeled] = 1\n",
    "      s[y_train==0] = negative_set\n",
    "  \n",
    "    print(\"image = {0}\".format(image))\n",
    "    print(\"Training set: total = {0}, positives = {1}, negatives = {2}, P_noisy = {3}, N_noisy = {4}\"\n",
    "      .format(len(X_train), sum(y_train), len(y_train)-sum(y_train), sum(s), len(s)-sum(s)))\n",
    "    print(\"Testing set:  total = {0}, positives = {1}, negatives = {2}\"\n",
    "      .format(len(X_test), sum(y_test), len(y_test) - sum(y_test)))\n",
    "\n",
    "\n",
    "    # Fit different models for PU learning\n",
    "    for key in ordered_models_keys:\n",
    "      fit_start_time = dt.now()\n",
    "      print(\"\\n\\nFitting {0} classifier. Default classifier is {1}.\".format(key, clf_type))\n",
    "\n",
    "      if clf_type == \"logreg\":\n",
    "        clf = LogisticRegression()\n",
    "      elif clf_type == \"cnn\":\n",
    "        from classifier_cnn import CNN\n",
    "        from keras import backend as K\n",
    "        K.clear_session()\n",
    "        clf = CNN(            \n",
    "            dataset_name = dataset, \n",
    "            num_category = 2, \n",
    "            epochs = epochs, \n",
    "            early_stopping = early_stopping, \n",
    "            verbose = 1,\n",
    "        )\n",
    "      else:\n",
    "        raise ValueError(\"clf_type must be either logreg or cnn for this testing file.\")\n",
    "        \n",
    "      ps1 = sum(s) / float(len(s))\n",
    "      py1 = sum(y_train) / float(len(y_train))\n",
    "      true_rh0 = pi1 * ps1 / float(1 - py1)\n",
    "      \n",
    "      model = get_model(\n",
    "        key = key,\n",
    "        rh1 = true_rh1,\n",
    "        rh0 = true_rh0,\n",
    "        clf = clf,\n",
    "      )\n",
    "  \n",
    "      try:\n",
    "        if key == \"True Classifier\":\n",
    "          model.fit(X_train, y_train)\n",
    "        elif key in [\"Rank Pruning\", \"Rank Pruning (noise rates given)\", \"Liu16 (noise rates given)\"]:\n",
    "          model.fit(X_train, s, pulearning = pulearning, cv_n_folds = cv_n_folds)\n",
    "        elif key in [\"Nat13 (noise rates given)\"]:\n",
    "          model.fit(X_train, s, pulearning = pulearning)\n",
    "        else: # Elk08, Baseline\n",
    "          model.fit(X_train, s)\n",
    "      \n",
    "        pred = model.predict(X_test)\n",
    "        # Produces only P(y=1|x) for pulearning models because they are binary\n",
    "        pred_prob = model.predict_proba(X_test) \n",
    "        pred_prob = pred_prob[:,1] if key == \"True Classifier\" else pred_prob\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics_dict = get_metrics(pred, pred_prob, y_test)\n",
    "        elapsed = (dt.now() - fit_start_time).total_seconds()\n",
    "\n",
    "        if verbose:\n",
    "          print(\"\\n{0} Model Performance at image {1}:\\n=================\\n\".format(key, image))\n",
    "          print(\"Time Required\", elapsed)\n",
    "          print(\"AUC:\", metrics_dict[\"AUC\"])\n",
    "          print(\"Error:\", metrics_dict[\"Error\"])\n",
    "          print(\"Precision:\", metrics_dict[\"Precision\"])\n",
    "          print(\"Recall:\", metrics_dict[\"Recall\"])\n",
    "          print(\"F1 score:\", metrics_dict[\"F1 score\"])\n",
    "          print(\"rh1:\", model.rh1 if hasattr(model, 'rh1') else None)\n",
    "          print(\"rh0:\", model.rh0 if hasattr(model, 'rh0') else None)\n",
    "          print()\n",
    "      \n",
    "        metrics_dict[\"image\"] = image\n",
    "        metrics_dict[\"time_seconds\"] = elapsed\n",
    "        metrics_dict[\"rh1\"] = model.rh1 if hasattr(model, 'rh1') else None\n",
    "        metrics_dict[\"rh0\"] = model.rh0 if hasattr(model, 'rh0') else None\n",
    "\n",
    "        # Append dictionary of error and loss metrics\n",
    "        if key not in data_all[\"metrics\"]:\n",
    "          data_all[\"metrics\"][key] = [metrics_dict]\n",
    "        else:\n",
    "          data_all[\"metrics\"][key].append(metrics_dict)\n",
    "        data_all[\"calculated\"][(key, image)] = True\n",
    "\n",
    "      except Exception as e:\n",
    "        msg = \"Error in {0}, image {1}, rh1 {2}, m {3}: {4}\\n\".format(key, image, true_rh1, pi1, e)\n",
    "        print(msg)\n",
    "        make_sure_path_exists(\"failed_models/\")\n",
    "        with open(\"failed_models/\" + key + \".txt\", \"ab\") as f:\n",
    "          f.write(msg)\n",
    "        if suppress_error:\n",
    "          continue\n",
    "        else:\n",
    "          raise\n",
    "  return data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[***] true_rh1 = 0.5\n",
      "[***] image = 0\n",
      "[***] pi1 = 0.5\n",
      "[***] downsample_ratio = 0.5\n",
      "[***] cifar TEST: One vs. Rest\n",
      "Missing 'cifar-10-batches-py' directory for CIFAR-10, downloading CIFAR-10 data (may take > 1 min)...\n",
      "Note that Python package \"archive\" must be installed to extract CIFAR-10 data.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named archive",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-da454fbd1491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mcv_n_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_n_folds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mpulearning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpulearning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m       )\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e927abbe4c56>\u001b[0m in \u001b[0;36mrun_test\u001b[0;34m(dataset, clf_type, epochs, true_rh1, downsample_ratio, ordered_models_keys, list_of_images, suppress_error, verbose, pi1, one_vs_rest, cv_n_folds, early_stopping, pulearning)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m# Load MNIST or CIFAR data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0;34m(\u001b[0m\u001b[0mX_train_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_original\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mX_train_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownsample_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cgn/Dropbox (MIT)/cgn/rankpruning/tutorial_and_testing/util.pyc\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_MNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cifar\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_CIFAR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cgn/Dropbox (MIT)/cgn/rankpruning/tutorial_and_testing/util.pyc\u001b[0m in \u001b[0;36mget_CIFAR\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# Download and extract CIFAR-10 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     download_and_extract(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \n\u001b[1;32m    205\u001b[0m       \"\", md5sum = \"c58f30108f718f92721af3b95e74349a\")\n",
      "\u001b[0;31mImportError\u001b[0m: No module named archive"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  image_index = int(sys.argv[1])\n",
    "except:\n",
    "  image_index = None\n",
    "\n",
    "try:\n",
    "  model_index = int(sys.argv[2])\n",
    "except:\n",
    "  model_index = None\n",
    "\n",
    "\n",
    "image_list = range(10)\n",
    "ordered_models_keys = [\n",
    "  \"Rank Pruning\",\n",
    "  \"Rank Pruning (noise rates given)\",\n",
    "  \"Elk08 (noise rates given)\",\n",
    "  \"Nat13 (noise rates given)\",\n",
    "  \"Liu16 (noise rates given)\",\n",
    "  \"Baseline\",\n",
    "  \"True Classifier\",\n",
    "]\n",
    "\n",
    "if image_index is not None:\n",
    "  # Select only the single element\n",
    "  # otherwise all images are tested.\n",
    "  image_list = [image_list[image_index]]\n",
    "if model_index is not None:\n",
    "  # Select only the single model\n",
    "  # otherwise all models are tested.\n",
    "  ordered_models_keys = [ordered_models_keys[model_index]]\n",
    "\n",
    "for image in image_list:\n",
    "  for pi1, true_rh1 in [(0.5, 0.5), (0.25, 0.25), (0.5, 0.0), (0.0, 0.5)]:    \n",
    "    for model in ordered_models_keys:\n",
    "      # Parameter settings:\n",
    "      dataset = \"mnist\" # choose between mnist and cifar\n",
    "      downsample_ratio = 0.5 # What fraction of data to keep for speed increase\n",
    "\n",
    "      # clf specific settings:\n",
    "      clf_type = \"logreg\" # \"logreg\" or \"cnn\"\n",
    "      epochs = 50\n",
    "      cv_n_folds = 3\n",
    "      early_stopping = True\n",
    "\n",
    "      # Other settings (currently need not change):\n",
    "      suppress_error = False\n",
    "      verbose = True\n",
    "      one_vs_rest = True # Default is True, False -> test one vs other \n",
    "      pulearning = (pi1 == 0)\n",
    "\n",
    "      print(\"[***]\", \"true_rh1 =\", true_rh1)\n",
    "      print(\"[***]\", \"image =\", image)\n",
    "      print(\"[***]\", \"pi1 =\", pi1)\n",
    "      print(\"[***]\", \"downsample_ratio =\", downsample_ratio)\n",
    "      print(\"[***] {0} TEST: One vs.\".format(dataset), \"Rest\" if one_vs_rest else \"Other\")\n",
    "\n",
    "      data_all = run_test(\n",
    "        dataset = dataset,\n",
    "        clf_type = clf_type,\n",
    "        epochs = epochs,\n",
    "        true_rh1 = true_rh1,\n",
    "        downsample_ratio = downsample_ratio,\n",
    "        ordered_models_keys = [model],\n",
    "        list_of_images = [image],\n",
    "        suppress_error = suppress_error,\n",
    "        verbose = verbose,\n",
    "        pi1 = pi1,\n",
    "        one_vs_rest = one_vs_rest,\n",
    "        cv_n_folds = cv_n_folds, \n",
    "        early_stopping = early_stopping,\n",
    "        pulearning = pulearning,\n",
    "      )\n",
    "\n",
    "      print(\"Completed: model\", model, \"and image\", image)\n",
    "      \n",
    "      # Before we store results, create folder if needed.\n",
    "      make_sure_path_exists(\"data/\")\n",
    "      pickle.dump(data_all, open(\"data/metrics_{0}_{1}_{2}_epochs_rh1_{3}_downsample_{4}_model_{5}_image_{6}_pi1_{7}.p\".format(dataset, clf_type, epochs, true_rh1, downsample_ratio, model, image, pi1),\"wb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
