{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression as logreg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rankpruning import assert_inputs_are_valid, compute_cv_predicted_probabilities as cv_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Elk08:\n",
    "  '''\n",
    "  Elk08 implements the algorithm described in Elkan and Noto (2008) for \n",
    "    positive-unlabeled learning (binary semi-supervised classification).\n",
    "  Positive-unlabeled learning is needed when you have only some positive labels in a\n",
    "    training set, and no negative labels.\n",
    "  Given any classifier having the predict_proba() method, an input feature matrix, X, \n",
    "    and a binary vector of labels, s, which may contain mislabeling, Elk08 \n",
    "    estimates the classifications that would be obtained if the hidden, true labels, y,\n",
    "    had instead been provided to the classifier during training.\n",
    "    \n",
    "  Parameters\n",
    "  ----------\n",
    "  clf : sklearn.classifier\n",
    "    Stores the classifier used by Elkan's method.\n",
    "    Default classifier used is logistic regression\n",
    "      \n",
    "  e1 : float\n",
    "    Estimate of P(s=1|y=1) can be passed in. If None, it is estimated.\n",
    "  '''\n",
    "  \n",
    "  def __init__(\n",
    "    self, \n",
    "    clf = None, \n",
    "    e1 = None, \n",
    "  ):\n",
    "\n",
    "    self.clf = logreg() if clf is None else clf\n",
    "    self.e1 = e1\n",
    "    \n",
    "    \n",
    "  def fit(self, X, s, prob_s_eq_1 = None, cv_n_folds = 3):\n",
    "    '''Train the classifier using X examples and s labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "      \n",
    "    s : np.array\n",
    "      A binary vector of labels, s, which may contain mislabeling\n",
    "      \n",
    "    prob_s_eq_1 : iterable (list or np.array)\n",
    "      The probability, for each example, whether it is s==1 P(s==1|x). \n",
    "      If you are not sure, leave prob_s_eq_q = None (default) and\n",
    "      it will be computed for you using cross-validation.\n",
    "      \n",
    "    cv_n_folds : int\n",
    "      The number of cross-validation folds used to compute\n",
    "      out-of-sample probabilities for each example in X.\n",
    "    '''\n",
    "    \n",
    "    assert_inputs_are_valid(X, s, prob_s_eq_1)\n",
    "    \n",
    "    if prob_s_eq_1 is None:\n",
    "      prob_s_eq_1 = cv_pred_proba(\n",
    "        X = X, \n",
    "        s = s, \n",
    "        clf = self.clf, \n",
    "        cv_n_folds = cv_n_folds,\n",
    "      )\n",
    "    \n",
    "    if self.e1 is None:  \n",
    "      self.e1 = np.mean(prob_s_eq_1[s==1])\n",
    "    \n",
    "    self.clf.fit(X, s)\n",
    "\n",
    "      \n",
    "  def predict(self, X):\n",
    "    '''Returns a binary vector of predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "    '''\n",
    "    \n",
    "    return np.array(self.clf.predict_proba(X)[:,1] / self.e1 > 0.5, dtype=int)\n",
    "  \n",
    "  \n",
    "  def predict_proba(self, X):\n",
    "    '''Returns a vector of probabilties for only P(y=1) for each example in X.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "    '''\n",
    "    \n",
    "    return self.clf.predict_proba(X)[:,1] / self.e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaselineNoisyPN:\n",
    "  '''BaselineNoisyPN fits the classifier using noisy labels (assumes s = y).\n",
    "  '''\n",
    "  \n",
    "  def __init__(self, clf = None):\n",
    "\n",
    "    # Stores the classifier used.\n",
    "    # Default classifier used is logistic regression\n",
    "    self.clf = logreg() if clf is None else clf\n",
    "  \n",
    "  \n",
    "  def fit(self, X, s):\n",
    "    '''Train the classifier clf with s labels.\n",
    "    \n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "    s : np.array\n",
    "      A binary vector of labels, s, which may contain mislabeling\n",
    "    '''\n",
    "    \n",
    "    assert_inputs_are_valid(X, s)\n",
    "    \n",
    "    self.clf.fit(X, s)\n",
    "        \n",
    "      \n",
    "  def predict(self, X):\n",
    "    '''Returns a binary vector of predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "    '''\n",
    "    \n",
    "    return self.clf.predict(X)\n",
    "    \n",
    "    \n",
    "  def predict_proba(self, X):\n",
    "    '''Returns a vector of probabilties for only P(y=1) for each example in X.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "    '''\n",
    "    \n",
    "    return self.clf.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaselinePU(BaselineNoisyPN):\n",
    "  '''BaselinePU is the simplest method for positive-unlabeled learning (binary \n",
    "  semi-supervised classification). It simpley assumes s = y.\n",
    "  \n",
    "  Positive-unlabeled learning is needed when you have only some positive labels in a\n",
    "    training set, and no negative labels.\n",
    "    \n",
    "  Given only X, an input feature matrix, and s, a binary vector\n",
    "    (1 if labeled (and therefore positive), 0 if unlabeled), the \n",
    "    goal is to infer the true label vector y and produce the correct classifier.\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loss_Reweighting_Base_Class(object):\n",
    "  '''This class provides a base class for the following models:\n",
    "  Liu16 - reweights the loss function using probabilities\n",
    "  Nat13unbiased - Natarajan et al. (2013) first method\n",
    "    Referred to as \"unbiased loss function\"\n",
    "  Nat13 - Natarajan et al. (2013) second method\n",
    "    Referred to as \"alpha weighted loss function\"\n",
    "  \n",
    "  Parameters \n",
    "  ----------\n",
    "  clf : sklearn.classifier or equivalent\n",
    "    Stores the classifier used.\n",
    "    Default classifier used is logistic regression.\n",
    "    \n",
    "  frac_pos2neg : float \n",
    "    Fraction of negative examples mislabeled as positive examples. Typically,\n",
    "    leave this set to its default value of None. Only provide this value if you know the\n",
    "    fraction of mislabeling already. This value is called rho1 in the literature.\n",
    "    \n",
    "  frac_neg2pos : float\n",
    "    Fraction of positive examples mislabeled as negative examples. Typically,\n",
    "    leave this set to its default value of None. Only provide this value if you know the\n",
    "    fraction of mislabeling already. This value is called rho0 in the literature.\n",
    "  '''\n",
    "  \n",
    "  \n",
    "  def __init__(self, frac_pos2neg, frac_neg2pos, clf = None):\n",
    "    \n",
    "    if frac_pos2neg is not None and frac_neg2pos is not None:\n",
    "      # Verify that rh1 + rh0 < 1 and pi0 + pi1 < 1.\n",
    "      if frac_pos2neg + frac_neg2pos >= 1:\n",
    "        raise Exception(\"frac_pos2neg + frac_neg2pos < 1 is \" + \\\n",
    "          \"necessary condition for noisy PN (binary) classification.\")\n",
    "    \n",
    "    self.rh1 = frac_pos2neg\n",
    "    self.rh0 = frac_neg2pos\n",
    "    self.clf = logreg() if clf is None else clf\n",
    "    \n",
    "    \n",
    "  def predict(self, X):\n",
    "    '''Returns a binary vector of predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "    '''\n",
    "    return self.clf.predict(X)\n",
    "  \n",
    "  \n",
    "  def predict_proba(self, X):\n",
    "    '''Returns a vector of probabilties for only P(y=1) for each example in X.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "    '''\n",
    "    \n",
    "    return self.clf.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Liu16(Loss_Reweighting_Base_Class):\n",
    "  '''Implements the Liu et al. (2016) using probability based P(s=1|x) \n",
    "  as sample weights for refitting.\n",
    "  '''\n",
    "  \n",
    "  def fit(\n",
    "    self, \n",
    "    X, \n",
    "    s, \n",
    "    pulearning = None, \n",
    "    prob_s_eq_1 = None,\n",
    "    cv_n_folds = 3,\n",
    "  ):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "      \n",
    "    s : np.array\n",
    "      A binary vector of labels, s, which may contain mislabeling\n",
    "      \n",
    "    pulearning : bool\n",
    "      Set to True if you wish to perform PU learning. PU learning assumes \n",
    "      that positive examples are perfectly labeled (contain no mislabeling)\n",
    "      and therefore frac_neg2pos = 0 (rh0 = 0). If\n",
    "      you are not sure, leave pulearning = None (default).\n",
    "      \n",
    "    prob_s_eq_1 : iterable (list or np.array)\n",
    "      The probability, for each example, whether it is s==1 P(s==1|x). \n",
    "      If you are not sure, leave prob_s_eq_q = None (default) and\n",
    "      it will be computed for you using cross-validation.\n",
    "      \n",
    "    cv_n_folds : int\n",
    "      The number of cross-validation folds used to compute\n",
    "      out-of-sample probabilities for each example in X.\n",
    "    '''\n",
    "    \n",
    "    # Check if we are in the PU learning setting.\n",
    "    if pulearning is None:\n",
    "      pulearning = (self.rh0 == 0)\n",
    "    \n",
    "    assert_inputs_are_valid(X, s, prob_s_eq_1)\n",
    "    \n",
    "    # Set rh0 = 0 if no negatives exist in P.\n",
    "    rh0 = 0.0 if pulearning else self.rh0\n",
    "    rh1 = self.rh1\n",
    "    \n",
    "    if prob_s_eq_1 is None:\n",
    "      prob_s_eq_1 = cv_pred_proba(\n",
    "        X = X, \n",
    "        s = s, \n",
    "        clf = self.clf, \n",
    "        cv_n_folds = cv_n_folds,\n",
    "      )\n",
    "    \n",
    "    # Liu2016 using probabilities \n",
    "    assert prob_s_eq_1 is not None, \"Error: prob_s_eq_1 is None type.\"\n",
    "    rho_s_opposite = np.ones(np.shape(prob_s_eq_1)) * rh0\n",
    "    rho_s_opposite[s==0] = rh1\n",
    "    sample_weight = (prob_s_eq_1 - rho_s_opposite) / prob_s_eq_1 / float(1 - rh1 - rh0)\n",
    "    self.clf.fit(X, s, sample_weight = sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Nat13(Loss_Reweighting_Base_Class):\n",
    "  '''Implements Natarajan et al. (2013) by optimizing w.r.t. the\n",
    "  alpha-weighted loss function (Eq. (1)).\n",
    "  '''\n",
    "  \n",
    "  def fit(self, X, s, pulearning = None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "      \n",
    "    s : np.array\n",
    "      A binary vector of labels, s, which may contain mislabeling\n",
    "      \n",
    "    pulearning : bool\n",
    "      Set to True if you wish to perform PU learning. PU learning assumes \n",
    "      that positive examples are perfectly labeled (contain no mislabeling)\n",
    "      and therefore frac_neg2pos = 0 (rh0 = 0). If\n",
    "      you are not sure, leave pulearning = None (default).\n",
    "    '''\n",
    "    \n",
    "    # Check if we are in the PU learning setting.\n",
    "    if pulearning is None:\n",
    "      pulearning = (self.rh0 == 0)\n",
    "    \n",
    "    assert_inputs_are_valid(X, s)\n",
    "    \n",
    "    # Set rh0 = 0 if no negatives exist in P.\n",
    "    rh0 = 0.0 if pulearning else self.rh0\n",
    "    rh1 = self.rh1\n",
    "\n",
    "    alpha = float(1 - rh1 + rh0) / 2\n",
    "    sample_weight = np.ones(np.shape(s)) * (1 - alpha)\n",
    "    sample_weight[s==0] = alpha\n",
    "    self.clf.fit(X, s, sample_weight = sample_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
