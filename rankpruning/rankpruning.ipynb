{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as logreg\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_NUM_PER_CLASS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relevant helper functions exposed to rankpruning module. \n",
    "\n",
    "def assert_inputs_are_valid(X, s, prob_s_eq_1 = None):\n",
    "  '''Checks that X, s, and prob_s_eq_1\n",
    "  are correctly formatted'''\n",
    "  \n",
    "  if prob_s_eq_1 is not None:\n",
    "    if not isinstance(prob_s_eq_1, (np.ndarray, np.generic)):\n",
    "      raise TypeError(\"prob_s_eq_1 should be a numpy array.\")\n",
    "    if len(prob_s_eq_1) != len(s):\n",
    "      raise ValueError(\"prob_s_eq_1 and s must have same length.\")\n",
    "    # Check for valid probablities.\n",
    "    for i in prob_s_eq_1:\n",
    "      if i < 0 or i > 1:\n",
    "        raise ValueError(\"Values in prob_s_eq_1 must be between 0 and 1.\")\n",
    "\n",
    "  if not isinstance(s, (np.ndarray, np.generic)):\n",
    "    raise TypeError(\"s should be a numpy array.\")\n",
    "  if not isinstance(X, (np.ndarray, np.generic)):\n",
    "    raise TypeError(\"X should be a numpy array.\")\n",
    "  for i in s:\n",
    "    if i < 0 or i > 1 or (i > 0 and i < 1):\n",
    "      raise ValueError(\"s should only contain 0 or 1 values.\")\n",
    "          \n",
    "\n",
    "def compute_conf_counts_noise_rates_from_probabilities(\n",
    "  s, \n",
    "  prob_s_eq_1, \n",
    "  positive_lb_threshold = None,\n",
    "  negative_ub_threshold = None,\n",
    "  verbose = False,\n",
    "):\n",
    "  '''Function to compute the rho hat (rh) confident counts\n",
    "  estimate of the noise rates from prob_s_eq_1 and s.\n",
    "\n",
    "  Important! This function assumes that prob_s_eq_1 are out-of-sample \n",
    "  holdout probabilities. This can be done with cross validation. If\n",
    "  the probabilities are not computed out-of-sample, overfitting may occur.\n",
    "\n",
    "  This function estimates rh1 (the fraction of pos examples mislabeled\n",
    "  as neg, frac_pos2neg) and  rh0 (the fraction of neg examples \n",
    "  mislabeled as pos, frac_neg2pos). \n",
    "  \n",
    "  The acronym 'rh' stands for rho hat, where rho is a greek symbol for\n",
    "  noise rate and hat tells us that the value is estimated, not necessarily\n",
    "  exact. Under certain conditions, estimates are exact, and in most\n",
    "  conditions, estimates are within one percent of the actual noise rates.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "\n",
    "    s : np.array\n",
    "      A binary vector of labels, s, which may contain mislabeling\n",
    "\n",
    "    prob_s_eq_1 : iterable (list or np.array)\n",
    "      The probability, for each example, whether it is s==1 P(s==1|x). \n",
    "      If you are not sure, leave prob_s_eq_q = None (default) and\n",
    "      it will be computed for you using cross-validation.\n",
    "      \n",
    "    positive_lb_threshold : float \n",
    "      P(s^=1|s=1). If an example has a predicted probability \"greater\" than \n",
    "      this threshold, it is counted as having hidden label y = 1. This is \n",
    "      not used for pruning, only for estimating the noise rates using \n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "      \n",
    "    negative_ub_threshold : float \n",
    "      P(s^=1|s=0). If an example has a predicted probability \"lower\" than\n",
    "      this threshold, it is counted as having hidden label y = 0. This is\n",
    "      not used for pruning, only for estimating the noise rates using\n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "\n",
    "    verbose : bool\n",
    "      Set to true if you wish to print additional information while running.\n",
    "  '''\n",
    "\n",
    "  # Estimate the probability thresholds for confident counting \n",
    "  if positive_lb_threshold is None:\n",
    "    positive_lb_threshold = np.mean(prob_s_eq_1[s == 1]) # P(s^=1|s=1)\n",
    "  \n",
    "  if negative_ub_threshold is None:\n",
    "    negative_ub_threshold = np.mean(prob_s_eq_1[s == 0]) # P(s^=1|s=0)\n",
    "    \n",
    "  # Estimate the number of confident examples having s = 0 and y = 1\n",
    "  N_most_positive_size = sum((prob_s_eq_1 >= positive_lb_threshold) & (s == 0)) \n",
    "\n",
    "  # Estimate the number of confident examples having s = 1 and y = 1\n",
    "  P_most_positive_size = sum((prob_s_eq_1 >= positive_lb_threshold) & (s == 1))\n",
    "\n",
    "  # Estimate the number of confident examples having s = 0 and y = 0\n",
    "  N_least_positive_size = sum((prob_s_eq_1 <= negative_ub_threshold) & (s == 0))\n",
    "\n",
    "  # Estimate the number of confident examples having s = 1 and y = 0\n",
    "  P_least_positive_size = sum((prob_s_eq_1 <= negative_ub_threshold) & (s == 1))\n",
    "  \n",
    "  if verbose:\n",
    "    print(\"N_most_positive_size\", N_most_positive_size)\n",
    "    print(\"P_most_positive_size\", P_most_positive_size)\n",
    "    print(\"N_least_positive_size\", N_least_positive_size)\n",
    "    print(\"P_least_positive_size\", P_least_positive_size)\n",
    "  \n",
    "  # Confident Counts Estimator for p(s=0|y=1) ~ |s=0 and y=1| / |y=1|\n",
    "  # Allow np.NaN when float(N_most_positive_size + P_most_positive_size) == 0\n",
    "  rh1_conf = N_most_positive_size / float(N_most_positive_size + P_most_positive_size)\n",
    "\n",
    "  # Confident Counts Estimator for p(s=1|y=0) ~ |s=1 and y=0| / |y=0|\n",
    "  # Allow np.NaN when float(N_least_positive_size + P_least_positive_size) == 0\n",
    "  rh0_conf = P_least_positive_size / float(N_least_positive_size + P_least_positive_size)\n",
    "  \n",
    "  # Ensure that rh1, rh0 are in proper range [0,1)\n",
    "  rh0_conf = min(max(rh0_conf, 0.0), 0.9999)\n",
    "  rh1_conf = min(max(rh1_conf, 0.0), 0.9999)\n",
    "\n",
    "  if verbose:\n",
    "    print(\"Est count of s = 1 and y = 1:\", P_most_positive_size)\n",
    "    print(\"Est count of s = 0 and y = 1:\", N_most_positive_size)\n",
    "    print(\"Est count of s = 1 and y = 0:\", P_least_positive_size)\n",
    "    print(\"Est count of s = 0 and y = 0:\", N_least_positive_size)\n",
    "    print(\"rh1_conf:\", rh1_conf)\n",
    "    print(\"rh0_conf:\", rh0_conf)\n",
    "\n",
    "  return rh1_conf, rh0_conf\n",
    "\n",
    "\n",
    "def compute_noise_rates_and_cv_pred_proba(\n",
    "  X, \n",
    "  s, \n",
    "  clf = logreg(),\n",
    "  cv_n_folds = 3,\n",
    "  positive_lb_threshold = None,\n",
    "  negative_ub_threshold = None,\n",
    "  verbose = False,\n",
    "):\n",
    "  '''This function computes the out-of-sample predicted \n",
    "  probability P(s=k|x) for every example x in X using cross\n",
    "  validation while also computing the confident counts noise\n",
    "  rates within each cross-validated subset and returning\n",
    "  the average noise rate across all examples. \n",
    "\n",
    "  This function estimates rh1 (the fraction of pos examples mislabeled\n",
    "  as neg, frac_pos2neg) and  rh0 (the fraction of neg examples \n",
    "  mislabeled as pos, frac_neg2pos). \n",
    "  \n",
    "  The acronym 'rh' stands for rho hat, where rho is a greek symbol for\n",
    "  noise rate and hat tells us that the value is estimated, not necessarily\n",
    "  exact. Under certain conditions, estimates are exact, and in most\n",
    "  conditions, estimates are within one percent of the actual noise rates.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "\n",
    "    s : np.array\n",
    "      A binary vector of labels, s, which may contain mislabeling\n",
    "\n",
    "    clf : sklearn.classifier or equivalent\n",
    "      Default classifier used is logistic regression. Assumes clf\n",
    "      has predict_proba() and fit() defined.\n",
    "\n",
    "    cv_n_folds : int\n",
    "      The number of cross-validation folds used to compute\n",
    "      out-of-sample probabilities for each example in X.\n",
    "      \n",
    "    positive_lb_threshold : float \n",
    "      P(s^=1|s=1). If an example has a predicted probability \"greater\" than \n",
    "      this threshold, it is counted as having hidden label y = 1. This is \n",
    "      not used for pruning, only for estimating the noise rates using \n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "      \n",
    "    negative_ub_threshold : float \n",
    "      P(s^=1|s=0). If an example has a predicted probability \"lower\" than\n",
    "      this threshold, it is counted as having hidden label y = 0. This is\n",
    "      not used for pruning, only for estimating the noise rates using\n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "\n",
    "    verbose : bool\n",
    "      Set to true if you wish to print additional information while running.\n",
    "  '''\n",
    "  \n",
    "  # Number of classes\n",
    "  K = len(np.unique(s))\n",
    "  # Number of training examples\n",
    "  N = len(s)\n",
    "\n",
    "  # Create cross-validation object for out-of-sample predicted probabilities.\n",
    "  # CV folds preserve the fraction of noisy positive and\n",
    "  # noisy negative examples in each class.\n",
    "  kf = StratifiedKFold(n_splits = cv_n_folds, shuffle = True)\n",
    "\n",
    "  # Intialize result storage and final prob_s array\n",
    "  rh1_per_cv_fold = []\n",
    "  rh0_per_cv_fold = []\n",
    "  prob_s = np.zeros((N, K))\n",
    "\n",
    "  # Split X and s into \"cv_n_folds\" stratified folds.\n",
    "  for k, (cv_train_idx, cv_holdout_idx) in enumerate(kf.split(X, s)):\n",
    "\n",
    "    # Select the training and holdout cross-validated sets.\n",
    "    X_train_cv, X_holdout_cv = X[cv_train_idx], X[cv_holdout_idx]\n",
    "    s_train_cv, s_holdout_cv = s[cv_train_idx], s[cv_holdout_idx]\n",
    "\n",
    "    # Fit the clf classifier to the training set and \n",
    "    # predict on the holdout set and update prob_s. \n",
    "    clf.fit(X_train_cv, s_train_cv)\n",
    "    prob_s_cv = clf.predict_proba(X_holdout_cv) # P(s = k|x) # [:,1]\n",
    "    prob_s[cv_holdout_idx] = prob_s_cv\n",
    "\n",
    "    # Compute and append the confident counts noise estimators\n",
    "    # to estimate the positive and negative mislabeling rates.\n",
    "    rh1_cv, rh0_cv = compute_conf_counts_noise_rates_from_probabilities(\n",
    "      s = s_holdout_cv, \n",
    "      prob_s_eq_1 = prob_s_cv[:,1], # P(s = 1|x) \n",
    "      positive_lb_threshold = positive_lb_threshold,\n",
    "      negative_ub_threshold = negative_ub_threshold,\n",
    "      verbose = verbose,\n",
    "    )\n",
    "    rh1_per_cv_fold.append(rh1_cv)\n",
    "    rh0_per_cv_fold.append(rh0_cv)\n",
    "\n",
    "  # Return mean rh, omitting nan or inf values, and prob_s\n",
    "  return (\n",
    "    _mean_without_nan_inf(rh1_per_cv_fold), \n",
    "    _mean_without_nan_inf(rh0_per_cv_fold), \n",
    "    prob_s,\n",
    "  )\n",
    "\n",
    "\n",
    "def compute_cv_predicted_probabilities(\n",
    "  X, \n",
    "  y, # labels, can be noisy (s) or not noisy (y).\n",
    "  clf = logreg(),\n",
    "  cv_n_folds = 3,\n",
    "  verbose = False,\n",
    "):\n",
    "  '''This function computes the out-of-sample predicted \n",
    "  probability [P(s=k|x)] for every example in X using cross\n",
    "  validation. Output is a np.array of shape (N, K) where N is \n",
    "  the number of training examples and K is the number of classes.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "\n",
    "    y : np.array\n",
    "      A binary vector of labels, y, which may or may not contain mislabeling\n",
    "\n",
    "    clf : sklearn.classifier or equivalent\n",
    "      Default classifier used is logistic regression. Assumes clf\n",
    "      has predict_proba() and fit() defined.\n",
    "\n",
    "    cv_n_folds : int\n",
    "      The number of cross-validation folds used to compute\n",
    "      out-of-sample probabilities for each example in X.\n",
    "\n",
    "    verbose : bool\n",
    "      Set to true if you wish to print additional information while running.\n",
    "  '''\n",
    "\n",
    "  return compute_noise_rates_and_cv_pred_proba(\n",
    "    X = X, \n",
    "    s = y, \n",
    "    clf = clf,\n",
    "    cv_n_folds = cv_n_folds,\n",
    "    verbose = verbose,\n",
    "  )[-1]\n",
    "\n",
    "\n",
    "def compute_conf_counts_noise_rates(\n",
    "  X, \n",
    "  s, \n",
    "  clf = logreg(),\n",
    "  cv_n_folds = 3,\n",
    "  positive_lb_threshold = None,\n",
    "  negative_ub_threshold = None,\n",
    "  verbose = False,\n",
    "):\n",
    "  '''Computes the rho hat (rh) confident counts estimate of the\n",
    "  noise rates from X and s.\n",
    "  \n",
    "  This function estimates rh1 (the fraction of pos examples mislabeled\n",
    "  as neg, frac_pos2neg) and  rh0 (the fraction of neg examples \n",
    "  mislabeled as pos, frac_neg2pos). \n",
    "  \n",
    "  The acronym 'rh' stands for rho hat, where rho is a greek symbol for\n",
    "  noise rate and hat tells us that the value is estimated, not necessarily\n",
    "  exact. Under certain conditions, estimates are exact, and in most\n",
    "  conditions, estimates are within one percent of the actual noise rates.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "\n",
    "    s : np.array\n",
    "      A binary vector of labels, s, which may contain mislabeling\n",
    "\n",
    "    clf : sklearn.classifier or equivalent\n",
    "      Default classifier used is logistic regression. Assumes clf\n",
    "      has predict_proba() and fit() defined.\n",
    "\n",
    "    cv_n_folds : int\n",
    "      The number of cross-validation folds used to compute\n",
    "      out-of-sample probabilities for each example in X.\n",
    "      \n",
    "    positive_lb_threshold : float \n",
    "      P(s^=1|s=1). If an example has a predicted probability \"greater\" than \n",
    "      this threshold, it is counted as having hidden label y = 1. This is \n",
    "      not used for pruning, only for estimating the noise rates using \n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "      \n",
    "    negative_ub_threshold : float \n",
    "      P(s^=1|s=0). If an example has a predicted probability \"lower\" than\n",
    "      this threshold, it is counted as having hidden label y = 0. This is\n",
    "      not used for pruning, only for estimating the noise rates using\n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "\n",
    "    verbose : bool\n",
    "      Set to true if you wish to print additional information while running.\n",
    "  '''\n",
    "\n",
    "  return compute_noise_rates_and_cv_pred_proba(\n",
    "    X = X, \n",
    "    s = s, \n",
    "    clf = clf,\n",
    "    cv_n_folds = cv_n_folds,\n",
    "    positive_lb_threshold = positive_lb_threshold,\n",
    "    negative_ub_threshold = negative_ub_threshold,\n",
    "    verbose = verbose,\n",
    "  )[:-1]\n",
    "\n",
    "\n",
    "def compute_ps1_py1_pi1_pi0(s, rh1, rh0):\n",
    "  '''Compute ps1 := P(s=1), py1 := P(y=1), and inverse noise rates pi1, pi0.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "\n",
    "  s : np.array\n",
    "    A binary vector of labels, s, which may contain mislabeling\n",
    "    \n",
    "  rh1 : float \n",
    "    P(s=0|y=1). Fraction of positive examples mislabeled as negative examples. \n",
    "    rh1 = frac_pos2neg.\n",
    "    \n",
    "  rh0 : float\n",
    "    P(s=1|y=0). Fraction of negative examples mislabeled as positive examples. \n",
    "    rh0 = frac_neg2pos.\n",
    "  '''\n",
    "  \n",
    "  # Compute ps1 := P(s=1), py1 := P(y=1), and inverse noise rates pi1, pi0\n",
    "  ps1 = sum(s) / float(len(s))\n",
    "  py1 = (ps1 - rh0) / float(1 - rh1 - rh0)\n",
    "  pi1 = rh0 * (1 - py1) / float(ps1)\n",
    "  pi0 = (rh1 * py1) / float(1 - ps1)\n",
    "    \n",
    "#     # Equivalently, we can compute pi1, pi0, and py1 this way as well, but there is no need:\n",
    "#     pi1 = rh0 * (1 - ps1 - rh1) / float(ps1) / float(1 - rh1 - rh0)\n",
    "#     pi0 = rh1 * (ps1 - rh0) / float(1 - ps1) / float(1 - rh1 - rh0)\n",
    "#     py1 = ps1 * (1 - pi1) + pi0 * (1 - ps1)\n",
    "    \n",
    "  # Ensure that pi1, and pi0 are in proper range [0,1)\n",
    "  pi1 = min(max(pi1, 0.0), 0.9999)\n",
    "  pi0 = min(max(pi0, 0.0), 0.9999)\n",
    "  \n",
    "  return ps1, py1, pi1, pi0\n",
    "    \n",
    "\n",
    "def get_noise_indices(\n",
    "  s, \n",
    "  prob_s_eq_1, \n",
    "  frac_of_noise = 1.0,\n",
    "  pi1 = None,\n",
    "  pi0 = None,\n",
    "  num_to_remove_per_class = None,\n",
    "  verbose = False,\n",
    "):\n",
    "  '''Returns the indices of most likely (confident) label errors in s. The\n",
    "  number of indices returned is specified by frac_of_noise. When \n",
    "  frac_of_noise = 1.0, all \"confidently\" estimated noise indices are returned.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "\n",
    "  s : np.array\n",
    "    A binary vector of labels, s, which may contain mislabeling\n",
    "\n",
    "  prob_s_eq_1 : iterable (list or np.array)\n",
    "    The probability, for each example, whether it is s==1 P(s==1|x). \n",
    "    If you are not sure, leave prob_s_eq_q = None (default) and\n",
    "    it will be computed for you using cross-validation.\n",
    "\n",
    "  frac_of_noise : float\n",
    "    When frac_of_noise = 1.0, return all \"confidently\" estimated noise indices.\n",
    "    Value in range (0, 1] that determines the fraction of noisy example \n",
    "    indices to return based on the following formula for example class k.\n",
    "    frac_of_noise * number_of_mislabeled_examples_in_class_k, or equivalently    \n",
    "    frac_of_noise * inverse_noise_rate_class_k * num_examples_with_s_equal_k\n",
    "\n",
    "  pi1 : float \n",
    "    P(y=0|s=1) Fraction of observed positive examples that are mislabeled. If None,\n",
    "    pi1 will be computed from prob_s_eq_1 and s.\n",
    "\n",
    "  pi0 : float\n",
    "    P(y=1|s=0). Fraction of observed negative examples that are mislabeled. If None,\n",
    "    pi0 will be computed from prob_s_eq_1 and s.\n",
    "    \n",
    "  num_to_remove_per_class : list of int of length K (# of classes)\n",
    "    e.g. num_to_remove_per_class = [5, 10] would return the indices of the 5 most\n",
    "    likely mislabeled examples in class s = 0, and the 10 most likely mislabeled \n",
    "    examples in class s = 1. List must be integers and be of length K (the number\n",
    "    of classes).\n",
    "\n",
    "  class_label : int (non-negative)\n",
    "    If set to 0 or 1, only return noise indicies for that class_label. By\n",
    "    default this is set to None, which returns a list of indices of noise\n",
    "    for each class.\n",
    "\n",
    "  verbose : bool\n",
    "    Set to true if you wish to print additional information while running.\n",
    "  '''\n",
    "  \n",
    "  size_P_noisy = sum(s == 1)\n",
    "  size_N_noisy = sum(s == 0)\n",
    "  \n",
    "  if pi1 is None or pi0 is None:\n",
    "    rh1, rh0 = compute_conf_counts_noise_rates_from_probabilities(s, prob_s_eq_1)\n",
    "    _, _, pi1, pi0 = compute_ps1_py1_pi1_pi0(s, rh1, rh0)\n",
    "  \n",
    "  if num_to_remove_per_class is None:\n",
    "    # Estimate k0 and k1 (number of non-confident examples to prune)\n",
    "    # When frac_of_noise = 1, k1 and k0 are the number of expected mislabeling errors.\n",
    "    k1 = size_P_noisy * pi1 * frac_of_noise\n",
    "    k0 = size_N_noisy * pi0 * frac_of_noise\n",
    "  else:\n",
    "    k1 = num_to_remove_per_class[1]\n",
    "    k0 = num_to_remove_per_class[0]\n",
    "  \n",
    "  # The number of examples to prune in P and N. Leave at least 10 examples.\n",
    "  k1 = max(min(int(k1), size_P_noisy - MIN_NUM_PER_CLASS), 0)\n",
    "  k0 = max(min(int(k0), size_N_noisy - MIN_NUM_PER_CLASS), 0)\n",
    "  \n",
    "  if verbose:\n",
    "    print('k1: ', k1, ', k0: ', k0)\n",
    "\n",
    "  # Peform Pruning with threshold probabilities from BFPRT algorithm in O(n)\n",
    "  # Don't prune if pi1 = 0 or there are not MIN_NUM_PER_CLASS in P_noisy\n",
    "  if (pi1 > 0 and size_P_noisy > MIN_NUM_PER_CLASS) or num_to_remove_per_class is not None:\n",
    "    kth_smallest = np.partition(prob_s_eq_1[s == 1], k1)[k1]\n",
    "  else:\n",
    "    kth_smallest = -1.0\n",
    "  # Don't prune if pi0 = 0 or there are not MIN_NUM_PER_CLASS in N_noisy\n",
    "  if (pi0 > 0 and size_N_noisy > MIN_NUM_PER_CLASS) or num_to_remove_per_class is not None:\n",
    "    kth_largest = -np.partition(-prob_s_eq_1[s == 0], k0)[k0] \n",
    "  else:\n",
    "    kth_largest = 2.0 \n",
    "  \n",
    "  if verbose:\n",
    "    print('kth_smallest: ', kth_smallest, ', kth_largest: ', kth_largest)\n",
    "\n",
    "  noise_mask = ((prob_s_eq_1 > kth_largest) & (s == 0)) | ((prob_s_eq_1 < kth_smallest) & (s == 1))\n",
    "  \n",
    "  return noise_mask  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _mean_without_nan_inf(arr, replacement = None):\n",
    "  '''Private helper method for computing the mean\n",
    "  of a numpy array or iterable by replacing NaN and inf\n",
    "  values with a replacement value or ignore those values\n",
    "  if replacement = None.\n",
    "\n",
    "  Parameters \n",
    "  ----------\n",
    "  arr : iterable (list or np.array)\n",
    "    Any iterable that may contain NaN or inf values.\n",
    "\n",
    "  replacement : float\n",
    "    Replace NaN and inf values in arr with this value.\n",
    "  '''\n",
    "  if replacement is not None:\n",
    "    return np.mean(\n",
    "      [replacement if math.isnan(x) or math.isinf(x) else x for x in arr]\n",
    "    )\n",
    "  \n",
    "  x_real = [x for x in arr if not math.isnan(x) and not math.isinf(x)]\n",
    "  \n",
    "  if len(x_real) == 0:\n",
    "      raise ValueError(\"All rho_conf estimates are NaN. Check that\" \\\n",
    "        \"positive_lb_threshold and negative_ub_threshold values are not\" \\\n",
    "        \"too extreme (near 1 or 0), resulting in division by zero.\")\n",
    "  else:\n",
    "    return np.mean([x for x in arr if not math.isnan(x) and not math.isinf(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RankPruning(object):\n",
    "  '''\n",
    "  Rank Pruning is a state-of-the-art algorithm (2017) for \n",
    "    binary semi-supervised classification P̃Ñ learning with signficant mislabeling in\n",
    "    both the noisy Negative (N) and noisy Positive (P) sets.\n",
    "  Rank Pruning also achieves state-of-the-art performance for positive-unlabeled\n",
    "    learning (PU learning) where a subset of positive examples is given and\n",
    "    all other examples are unlabeled and assumed to be negative examples.\n",
    "  Rank Pruning works by \"learning from confident examples.\" Confident examples are\n",
    "    identified as those examples with predicted probability near their training label.\n",
    "  Given any classifier having the predict_proba() method, an input feature matrix, X, \n",
    "    and a binary vector of labels, s, which may contain mislabeling, Rank Pruning \n",
    "    estimates the classifications that would be obtained if the hidden, true labels, y,\n",
    "    had instead been provided to the classifier during training.\n",
    "  \n",
    "  Parameters \n",
    "  ----------\n",
    "  clf : sklearn.classifier or equivalent\n",
    "    Stores the classifier used in Rank Pruning.\n",
    "    Default classifier used is logistic regression.\n",
    "    \n",
    "  frac_pos2neg : float \n",
    "    P(s=0|y=1). Fraction of positive examples mislabeled as negative examples. Typically,\n",
    "    leave this set to its default value of None. Only provide this value if you know the\n",
    "    fraction of mislabeling already. This value is called rho1 in the literature.\n",
    "    \n",
    "  frac_neg2pos : float\n",
    "    P(s=1|y=0). Fraction of negative examples mislabeled as positive examples. Typically,\n",
    "    leave this set to its default value of None. Only provide this value if you know the\n",
    "    fraction of mislabeling already. This value is called rho0 in the literature.\n",
    "  '''\n",
    "  \n",
    "  \n",
    "  def __init__(self,\n",
    "    frac_pos2neg = None,\n",
    "    frac_neg2pos = None,\n",
    "    clf = None,\n",
    "  ):\n",
    "    \n",
    "    if frac_pos2neg is not None and frac_neg2pos is not None:\n",
    "      # Verify that rh1 + rh0 < 1 and pi0 + pi1 < 1.\n",
    "      if frac_pos2neg + frac_neg2pos >= 1:\n",
    "        raise Exception(\"frac_pos2neg + frac_neg2pos < 1 is \" + \\\n",
    "          \"a necessary condition for Rank Pruning.\")\n",
    "    \n",
    "    self.rh1 = frac_pos2neg\n",
    "    self.rh0 = frac_neg2pos\n",
    "    self.clf = logreg() if clf is None else clf\n",
    "  \n",
    "  \n",
    "  def get_fraction_of_positives_mislabeled_as_negative(self):\n",
    "    '''Accessor method for inverse positive noise rate.'''\n",
    "    return self.rh1\n",
    "  \n",
    "  \n",
    "  def get_fraction_of_negatives_mislabeled_as_positive(self):\n",
    "    '''Accessor method for inverse negative noise rate.'''\n",
    "    return self.rh0\n",
    "    \n",
    "  \n",
    "  def get_fraction_mislabeling_in_positive_set(self):\n",
    "    '''Accessor method for positive noise rate.'''\n",
    "    return self.pi1\n",
    "  \n",
    "  \n",
    "  def get_fraction_mislabeling_in_negative_set(self):\n",
    "    '''Accessor method for negative noise rate.'''\n",
    "    return self.pi0\n",
    "  \n",
    "  \n",
    "  def fit(\n",
    "    self, \n",
    "    X,\n",
    "    s,\n",
    "    cv_n_folds = 3,\n",
    "    pulearning = None,\n",
    "    prob_s_eq_1 = None,\n",
    "    positive_lb_threshold = None,\n",
    "    negative_ub_threshold = None,\n",
    "    verbose = False,\n",
    "  ):\n",
    "    '''This method implements the Rank Pruning mantra 'learning with confident examples.'\n",
    "    This function fits the classifer (self.clf) to (X, s) accounting for the noise in\n",
    "    both the positive and negative sets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "      \n",
    "    s : np.array\n",
    "      A binary vector of labels, s, which may contain mislabeling\n",
    "      \n",
    "    cv_n_folds : int\n",
    "      The number of cross-validation folds used to compute\n",
    "      out-of-sample probabilities for each example in X.\n",
    "      \n",
    "    pulearning : bool\n",
    "      Set to True if you wish to perform PU learning. PU learning assumes \n",
    "      that positive examples are perfectly labeled (contain no mislabeling)\n",
    "      and therefore frac_neg2pos = 0 (rh0 = 0). If\n",
    "      you are not sure, leave pulearning = None (default).\n",
    "      \n",
    "    prob_s_eq_1 : iterable (list or np.array)\n",
    "      The probability, for each example, whether it is s==1 P(s==1|x). \n",
    "      If you are not sure, leave prob_s_eq_q = None (default) and\n",
    "      it will be computed for you using cross-validation.\n",
    "      \n",
    "    positive_lb_threshold : float \n",
    "      P(s^=1|s=1). If an example has a predicted probability \"greater\" than \n",
    "      this threshold, it is counted as having hidden label y = 1. This is \n",
    "      not used for pruning, only for estimating the noise rates using \n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "      \n",
    "    negative_ub_threshold : float \n",
    "      P(s^=1|s=0). If an example has a predicted probability \"lower\" than\n",
    "      this threshold, it is counted as having hidden label y = 0. This is\n",
    "      not used for pruning, only for estimating the noise rates using\n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "      \n",
    "    verbose : bool\n",
    "      Set to true if you wish to print additional information while running.\n",
    "    '''\n",
    "    \n",
    "    # Check if we are in the PU learning setting.\n",
    "    if pulearning is None:\n",
    "      pulearning = (self.rh0 == 0)\n",
    "    \n",
    "    assert_inputs_are_valid(X, s, prob_s_eq_1)\n",
    "    \n",
    "    # Compute noise rates (fraction of mislabeling) for the\n",
    "    # positive and negative sets. Also compute P(s=1|x) if needed.\n",
    "    if prob_s_eq_1 is None or self.rh1 is None or self.rh0 is None:\n",
    "      if prob_s_eq_1 is None:\n",
    "        rh1, rh0, prob_s = \\\n",
    "        compute_noise_rates_and_cv_pred_proba(\n",
    "          X = X, \n",
    "          s = s, \n",
    "          clf = self.clf,\n",
    "          cv_n_folds = cv_n_folds,\n",
    "          positive_lb_threshold = positive_lb_threshold,\n",
    "          negative_ub_threshold = negative_ub_threshold,\n",
    "          verbose = verbose,\n",
    "        )\n",
    "        # Only P(s=1|x) is needed for binary case\n",
    "        prob_s_eq_1 = prob_s[:,1]\n",
    "        del prob_s\n",
    "      else:\n",
    "        rh1, rh0 = \\\n",
    "        compute_conf_counts_noise_rates_from_probabilities(\n",
    "          s = s, \n",
    "          prob_s_eq_1 = prob_s_eq_1,\n",
    "          positive_lb_threshold = positive_lb_threshold,\n",
    "          negative_ub_threshold = negative_ub_threshold, \n",
    "          verbose = verbose,\n",
    "        )\n",
    "    \n",
    "    # Set the noise rates to user-provided values, if provided.\n",
    "    self.rh1 = self.rh1 if self.rh1 is not None else rh1\n",
    "    self.rh0 = self.rh0 if self.rh0 is not None else rh0\n",
    "    \n",
    "    # Set rh0 if we are in the pulearning setting\n",
    "    self.rh0 = 0.0 if pulearning else self.rh0\n",
    "    \n",
    "    # Compute ps1 := P(s=1), py1 := P(y=1), and inverse noise rates pi1, pi0\n",
    "    self.ps1, self.py1, self.pi1, self.pi0 = compute_ps1_py1_pi1_pi0(s, self.rh1, self.rh0)\n",
    "      \n",
    "    # Get the indices of the examples we wish to prune\n",
    "    prune_mask = get_noise_indices(s, prob_s_eq_1, pi1 = self.pi1, pi0 = self.pi0)\n",
    "    \n",
    "    X_mask = ~prune_mask\n",
    "    X_pruned = X[X_mask]\n",
    "    s_pruned = s[X_mask]\n",
    "    \n",
    "    # Re-weight examples in the loss function for the final fitting\n",
    "    # s.t. the \"apparent\" original number of examples in P and N\n",
    "    # is preserved, even though the pruned set may differ.\n",
    "    sample_weight = np.ones(np.shape(s_pruned)) / float(1 - self.rh1)\n",
    "    sample_weight[s_pruned == 0] = 1.0 / float(1 - self.rh0)\n",
    "    \n",
    "    self.clf.fit(X_pruned, s_pruned, sample_weight = sample_weight)\n",
    "    \n",
    "  def predict(self, X):\n",
    "    '''\n",
    "    Returns a binary vector of predictions.\n",
    "    '''\n",
    "    return self.clf.predict(X)\n",
    "  \n",
    "  \n",
    "  def predict_proba(self, X):\n",
    "    '''\n",
    "    Returns a vector of probabilties for only P(y=1) for each example in X.\n",
    "    '''\n",
    "    \n",
    "    return self.clf.predict_proba(X)[:,1]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
